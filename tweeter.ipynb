{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. There are no coincidences\n",
      "2. Your feedback is appreciated, now pay $8\n",
      "3. What today’s election feels like!\n",
      "\n",
      "Team America \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Setup the driver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run browser in headless mode (no GUI)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "def scrape_twitter_account(username, scrolls=3):\n",
    "    url = f'https://twitter.com/{username}'\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Let the page load initially\n",
    "\n",
    "    # Scroll to load tweets\n",
    "    for _ in range(scrolls):\n",
    "        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)  # Scroll down to load more tweets\n",
    "        time.sleep(2)  # Wait for content to load after each scroll\n",
    "    \n",
    "    # Wait until the first tweet or specific element is visible to make sure tweets are loaded\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//article[@role='article']\")))\n",
    "    except:\n",
    "        print(\"Error: Tweets not loaded in time.\")\n",
    "\n",
    "    # Get page source after scrolling and loading tweets\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Extract tweet data\n",
    "    tweet_data = []\n",
    "    tweets = soup.find_all('article', {'role': 'article'})  # Use article tags to find tweets\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        content = tweet.find('div', {'lang': True})\n",
    "        if content:\n",
    "            tweet_data.append(content.get_text())\n",
    "\n",
    "    return tweet_data\n",
    "\n",
    "# Example: Scrape tweets from a specific Twitter account (e.g., @Madhupriya98682)\n",
    "username = '@elonmusk'\n",
    "tweets = scrape_twitter_account(username, scrolls=5)\n",
    "\n",
    "# Print the tweets\n",
    "for idx, tweet in enumerate(tweets, 1):\n",
    "    print(f\"{idx}. {tweet}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 6 tweets...\n",
      "Scraped 10 tweets...\n",
      "No more new tweets. Ending scraping.\n",
      "1. Halloween with my Mom\n",
      "2. El burro sabe mas que Maduro\n",
      "3. Since I’ve been asked a lot:\n",
      "\n",
      "Buy stock in several companies that make products & services that *you* believe in.\n",
      "\n",
      "Only sell if you think their products & services are trending worse. Don’t panic when the market does.\n",
      "\n",
      "This will serve you well in the long-term.\n",
      "4. To all complainers, please continue complaining, but it will cost $8\n",
      "5. \n",
      "6. And lead us not into temptation …\n",
      "7. Biden’s mistake is that he thinks he was elected to transform the country, but actually everyone just wanted less drama\n",
      "8. There are no coincidences\n",
      "9. Your feedback is appreciated, now pay $8\n",
      "10. What today’s election feels like!\n",
      "\n",
      "Team America \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Set up Chrome WebDriver (with headless mode for non-GUI operation)\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "def scrape_twitter_account(username, scrolls=20, max_tweets=100):\n",
    "    url = f'https://twitter.com/{username}'\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Let the page load initially\n",
    "\n",
    "    # Wait for the page to load\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//article[@role='article']\")))\n",
    "    except:\n",
    "        print(\"Error: Tweets not loaded in time.\")\n",
    "    \n",
    "    # List to store the tweet texts\n",
    "    tweet_data = []\n",
    "    \n",
    "    # Get the initial tweet count\n",
    "    previous_tweet_count = 0\n",
    "    \n",
    "    while len(tweet_data) < max_tweets:\n",
    "        # Scroll to load more tweets\n",
    "        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "        time.sleep(3)  # Wait for new tweets to load\n",
    "        \n",
    "        # Get page source and extract tweets\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        tweets = soup.find_all('article', {'role': 'article'})\n",
    "        \n",
    "        # Extract tweet content and add to the list\n",
    "        for tweet in tweets:\n",
    "            content = tweet.find('div', {'lang': True})\n",
    "            if content:\n",
    "                tweet_text = content.get_text()\n",
    "                if tweet_text not in tweet_data:  # Avoid duplicate tweets\n",
    "                    tweet_data.append(tweet_text)\n",
    "        \n",
    "        # If the number of tweets doesn't increase, break (prevent infinite loop)\n",
    "        if len(tweet_data) == previous_tweet_count:\n",
    "            print(\"No more new tweets. Ending scraping.\")\n",
    "            break\n",
    "        \n",
    "        # Update the previous tweet count\n",
    "        previous_tweet_count = len(tweet_data)\n",
    "\n",
    "        print(f\"Scraped {len(tweet_data)} tweets...\")\n",
    "\n",
    "    return tweet_data\n",
    "\n",
    "# Example: Scrape tweets from a specific Twitter account (e.g., @elonmusk)\n",
    "username = 'elonmusk'\n",
    "tweets = scrape_twitter_account(username, scrolls=20, max_tweets=100)\n",
    "\n",
    "# Print the tweets\n",
    "for idx, tweet in enumerate(tweets, 1):\n",
    "    print(f\"{idx}. {tweet}\")\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10. What today’s election feels like!\n",
      "\n",
      "Team America \n"
     ]
    }
   ],
   "source": [
    "print(f\"{idx}. {tweet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Halloween with my Mom\n",
      "2. El burro sabe mas que Maduro\n",
      "3. Since Ive been asked a lot Buy stock in several companies that make products services that you believe in Only sell if you think their products services are trending worse Dont panic when the market does This will serve you well in the longterm\n",
      "4. To all complainers please continue complaining but it will cost 8\n",
      "5. \n",
      "6. And lead us not into temptation\n",
      "7. Bidens mistake is that he thinks he was elected to transform the country but actually everyone just wanted less drama\n",
      "8. There are no coincidences\n",
      "9. Your feedback is appreciated now pay 8\n",
      "10. What todays election feels like Team America\n"
     ]
    }
   ],
   "source": [
    "#CLEAN THE TWEET DARA\n",
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    # Remove mentions (e.g., @username)\n",
    "    tweet = re.sub(r\"@\\w+\", \"\", tweet)\n",
    "    # Remove hashtags (optional, you may want to keep them)\n",
    "    tweet = re.sub(r\"#\\w+\", \"\", tweet)\n",
    "    # Remove non-alphanumeric characters (e.g., punctuation)\n",
    "    tweet = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", tweet)\n",
    "    # Remove extra spaces\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "# Clean the scraped tweets\n",
    "cleaned_tweets = [clean_tweet(tweet) for tweet in tweets]\n",
    "\n",
    "# Print the cleaned tweets\n",
    "for idx, tweet in enumerate(cleaned_tweets, 1):\n",
    "    print(f\"{idx}. {tweet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Halloween with my Mom - Sentiment: Neutral\n",
      "2. El burro sabe mas que Maduro - Sentiment: Neutral\n",
      "3. Since Ive been asked a lot Buy stock in several companies that make products services that you believe in Only sell if you think their products services are trending worse Dont panic when the market does This will serve you well in the longterm - Sentiment: Positive\n",
      "4. To all complainers please continue complaining but it will cost 8 - Sentiment: Negative\n",
      "5.  - Sentiment: Neutral\n",
      "6. And lead us not into temptation - Sentiment: Neutral\n",
      "7. Bidens mistake is that he thinks he was elected to transform the country but actually everyone just wanted less drama - Sentiment: Negative\n",
      "8. There are no coincidences - Sentiment: Negative\n",
      "9. Your feedback is appreciated now pay 8 - Sentiment: Positive\n",
      "10. What todays election feels like Team America - Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "#Sentiment Analysis Code:\n",
    "\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(tweet):\n",
    "    # Analyze the sentiment of the tweet\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)\n",
    "    compound_score = sentiment_score['compound']  # Overall sentiment score\n",
    "    \n",
    "    # Classify sentiment based on compound score\n",
    "    if compound_score >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif compound_score <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Analyze sentiment for each cleaned tweet\n",
    "sentiments = [analyze_sentiment(tweet) for tweet in cleaned_tweets]\n",
    "\n",
    "# Print tweets with sentiment\n",
    "for idx, (tweet, sentiment) in enumerate(zip(cleaned_tweets, sentiments), 1):\n",
    "    print(f\"{idx}. {tweet} - Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Mining - Extract Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords:\n",
      "coincidences: 1.0\n",
      "halloween: 0.7071067811865475\n",
      "lead: 0.7071067811865475\n",
      "mom: 0.7071067811865475\n",
      "temptation: 0.7071067811865475\n",
      "appreciated: 0.5773502691896257\n",
      "feedback: 0.5773502691896257\n",
      "pay: 0.5773502691896257\n",
      "complainers: 0.5\n",
      "complaining: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extract_keywords(tweets, top_n=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(tweets)\n",
    "    \n",
    "    # Get the words with the highest TF-IDF scores\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = X.sum(axis=0).A1\n",
    "    \n",
    "    # Sort by the score to get the top N words\n",
    "    word_scores = list(zip(feature_names, scores))\n",
    "    sorted_word_scores = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_word_scores[:top_n]\n",
    "\n",
    "# Extract the top 10 keywords from cleaned tweets\n",
    "top_keywords = extract_keywords(cleaned_tweets, top_n=10)\n",
    "\n",
    "print(\"Top Keywords:\")\n",
    "for word, score in top_keywords:\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_to_csv(tweets, sentiments, filename='tweets.csv'):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Tweet\", \"Sentiment\"])\n",
    "        for tweet, sentiment in zip(tweets, sentiments):\n",
    "            writer.writerow([tweet, sentiment])\n",
    "\n",
    "# Save the cleaned tweets and their sentiment to CSV\n",
    "save_to_csv(cleaned_tweets, sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_to_json(tweets, sentiments, filename='tweets.json'):\n",
    "    data = [{\"tweet\": tweet, \"sentiment\": sentiment} for tweet, sentiment in zip(tweets, sentiments)]\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Save to JSON\n",
    "save_to_json(cleaned_tweets, sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 5 tweets...\n",
      "No more new tweets. Ending scraping.\n",
      "1. And lead us not into temptation …\n",
      "2. Biden’s mistake is that he thinks he was elected to transform the country, but actually everyone just wanted less drama\n",
      "3. There are no coincidences\n",
      "4. Your feedback is appreciated, now pay $8\n",
      "5. What today’s election feels like!\n",
      "\n",
      "Team America \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Set up Chrome WebDriver (with headless mode for non-GUI operation)\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "def scrape_twitter_account(username, max_tweets=30, scrolls=10):\n",
    "    url = f'https://twitter.com/{username}'\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Let the page load initially\n",
    "\n",
    "    # Wait for the first tweet to load\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//article[@role='article']\")))\n",
    "    except:\n",
    "        print(\"Error: Tweets not loaded in time.\")\n",
    "    \n",
    "    tweet_data = []\n",
    "    previous_tweet_count = 0\n",
    "\n",
    "    while len(tweet_data) < max_tweets:\n",
    "        # Scroll to load more tweets\n",
    "        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "        time.sleep(3)  # Wait for new tweets to load\n",
    "\n",
    "        # Get the page source after scrolling\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        tweets = soup.find_all('article', {'role': 'article'})\n",
    "\n",
    "        # Extract tweet text and append it to the list\n",
    "        for tweet in tweets:\n",
    "            content = tweet.find('div', {'lang': True})\n",
    "            if content:\n",
    "                tweet_text = content.get_text()\n",
    "                if tweet_text not in tweet_data:  # Avoid duplicates\n",
    "                    tweet_data.append(tweet_text)\n",
    "        \n",
    "        # If no new tweets are loaded, break the loop\n",
    "        if len(tweet_data) == previous_tweet_count:\n",
    "            print(\"No more new tweets. Ending scraping.\")\n",
    "            break\n",
    "        \n",
    "        previous_tweet_count = len(tweet_data)\n",
    "        print(f\"Scraped {len(tweet_data)} tweets...\")\n",
    "\n",
    "    return tweet_data\n",
    "\n",
    "# Example: Scrape tweets from a specific Twitter account (e.g., @elonmusk)\n",
    "username = 'elonmusk'\n",
    "tweets = scrape_twitter_account(username, max_tweets=50, scrolls=20)\n",
    "\n",
    "# Print the tweets\n",
    "for idx, tweet in enumerate(tweets, 1):\n",
    "    print(f\"{idx}. {tweet}\")\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
